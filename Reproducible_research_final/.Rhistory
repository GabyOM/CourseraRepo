head(polls)
polls$remain
names(polls) <- c("dates", "remain", "leave", "undecided", "lead", "samplesize", "pollster", "poll_type", "notes")
polls <- polls[str_detect(polls$remain, "%"), -9]
nrow(polls)
as.numeric(str_remove(polls$remain, "%"))
as.numeric(polls$remain)/100
parse_number(polls$remain)
str_remove(polls$remain, "%")/100
as.numeric(str_replace(polls$remain, "%", ""))/100
parse_number(polls$remain)/100
function_name(polls$undecided, "arg1", "arg2")
head(polls)
stringr::str_replace_all(polls$undecided, "N/A", "0")
temp <- str_extract_all(polls$dates, _____)
end_date <- sapply(temp, function(x) x[length(x)]) # take last element (handles polls that cross month boundaries)
temp <- str_extract_all(polls$dates, "\\d?\\s[a-zA-Z]?")
end_date <- sapply(temp, function(x) x[length(x)]) # take last element (handles polls that cross month boundaries)
temp
head(temp)
temp <- str_extract_all(polls$dates, "\\d+\\s[a-zA-Z]+")
end_date <- sapply(temp, function(x) x[length(x)]) # take last element (handles polls that cross month boundaries)
head(temp)
temp <- str_extract_all(polls$dates, "\\d+\\s[A-Z]+")
end_date <- sapply(temp, function(x) x[length(x)]) # take last element (handles polls that cross month boundaries)
head(temp)
temp <- str_extract_all(polls$dates, "[0-9]+\\s[a-zA-Z]+")
end_date <- sapply(temp, function(x) x[length(x)]) # take last element (handles polls that cross month boundaries)
head(temp)
temp <- str_extract_all(polls$dates, "\\d{1,2}\\s[a-zA-Z]+")
end_date <- sapply(temp, function(x) x[length(x)]) # take last element (handles polls that cross month boundaries)
head(temp)
temp <- str_extract_all(polls$dates, "\\d{1,2}[a-zA-Z]+")
end_date <- sapply(temp, function(x) x[length(x)]) # take last element (handles polls that cross month boundaries)
head(temp)
temp <- str_extract_all(polls$dates, "\\d+\\s[a-zA-Z]{3,5}")
end_date <- sapply(temp, function(x) x[length(x)]) # take last element (handles polls that cross month boundaries)
head(temp)
library(tidyverse)
library(dslabs)
data("polls_us_election_2016")
polls_us_election_2016$startdate %>% head
class(polls_us_election_2016$startdate)
as.numeric(polls_us_election_2016$startdate) %>% head
polls_us_election_2016 %>% filter(pollster == "Ipsos" & state =="U.S.") %>%
ggplot(aes(startdate, rawpoll_trump)) +
geom_line()
library(lubridate)
set.seed(2)
dates <- sample(polls_us_election_2016$startdate, 10) %>% sort
dates
data.frame(date = dates,
month = month(dates),
day = day(dates),
year = year(dates))
month(dates, label = TRUE)
x <- c(20090101, "2009-01-02", "2009 01 03", "2009-1-4",
"2009-1, 5", "Created on 2009 1 6", "200901 !!! 07")
ymd(x)
x <- "09/01/02"
ymd(x)
mdy(x)
ydm(x)
myd(x)
dmy(x)
dym(x)
now()    # current time in your time zone
now("GMT")    # current time in GMT
now() %>% hour()    # current hour
now() %>% minute()    # current minute
now() %>% second()    # current second
# parse time
x <- c("12:34:56")
hms(x)
x <- "Nov/2/2012 12:34:56"
mdy_hms(x)
library(tidyverse)
library(ggplot2)
library(lubridate)
library(tidyr)
library(scales)
set.seed(1)
url <- 'http://www.trumptwitterarchive.com/data/realdonaldtrump/%s.json'
trump_tweets <- map(2009:2017, ~sprintf(url, .x)) %>%
map_df(jsonlite::fromJSON, simplifyDataFrame = TRUE) %>%
filter(!is_retweet & !str_detect(text, '^"')) %>%
mutate(created_at = parse_date_time(created_at, orders = "a b! d! H!:M!:S! z!* Y!", tz="EST"))
library(dslabs)
data("trump_tweets")
head(trump_tweets)
names(trump_tweets)
trump_tweets %>% select(text) %>% head
trump_tweets %>% count(source) %>% arrange(desc(n))
trump_tweets %>%
extract(source, "source", "Twitter for (.*)") %>%
count(source)
campaign_tweets <- trump_tweets %>%
extract(source, "source", "Twitter for (.*)") %>%
filter(source %in% c("Android", "iPhone") &
created_at >= ymd("2015-06-17") &
created_at < ymd("2016-11-08")) %>%
filter(!is_retweet) %>%
arrange(created_at)
ds_theme_set()
campaign_tweets %>%
mutate(hour = hour(with_tz(created_at, "EST"))) %>%
count(source, hour) %>%
group_by(source) %>%
mutate(percent = n / sum(n)) %>%
ungroup %>%
ggplot(aes(hour, percent, color = source)) +
geom_line() +
geom_point() +
scale_y_continuous(labels = percent_format()) +
labs(x = "Hour of day (EST)",
y = "% of tweets",
color = "")
library(tidytext)
install.packages("tidytext")
library(tidytext)
example <- data_frame(line = c(1, 2, 3, 4),
text = c("Roses are red,", "Violets are blue,", "Sugar is sweet,", "And so are you."))
example
example %>% unnest_tokens(word, text)
campaign_tweets$text[i]
campaign_tweets[i,] %>%
unnest_tokens(word, text) %>%
select(word)
i <- 3008
campaign_tweets$text[i]
campaign_tweets[i,] %>%
unnest_tokens(word, text) %>%
select(word)
pattern <- "([^A-Za-z\\d#@']|'(?![A-Za-z\\d#@]))"
campaign_tweets[i,] %>%
unnest_tokens(word, text, token = "regex", pattern = pattern) %>%
select(word)
campaign_tweets[i,] %>%
mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
unnest_tokens(word, text, token = "regex", pattern = pattern) %>%
select(word)
tweet_words <- campaign_tweets %>%
mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
unnest_tokens(word, text, token = "regex", pattern = pattern)
tweet_words %>%
count(word) %>%
arrange(desc(n))
stop_words
tweet_words <- campaign_tweets %>%
mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
unnest_tokens(word, text, token = "regex", pattern = pattern) %>%
filter(!word %in% stop_words$word )
tweet_words %>%
count(word) %>%
top_n(10, n) %>%
mutate(word = reorder(word, n)) %>%
arrange(desc(n))
tweet_words <- campaign_tweets %>%
mutate(text = str_replace_all(text, "https://t.co/[A-Za-z\\d]+|&amp;", ""))  %>%
unnest_tokens(word, text, token = "regex", pattern = pattern) %>%
filter(!word %in% stop_words$word &
!str_detect(word, "^\\d+$")) %>%
mutate(word = str_replace(word, "^'", ""))
android_iphone_or <- tweet_words %>%
count(word, source) %>%
spread(source, n, fill = 0) %>%
mutate(or = (Android + 0.5) / (sum(Android) - Android + 0.5) /
( (iPhone + 0.5) / (sum(iPhone) - iPhone + 0.5)))
android_iphone_or %>% arrange(desc(or))
android_iphone_or %>% arrange(or)
android_iphone_or %>% filter(Android+iPhone > 100) %>%
arrange(desc(or))
android_iphone_or %>% filter(Android+iPhone > 100) %>%
arrange(or)
sentiments
get_sentiments("bing")
get_sentiments("afinn")
install.packages("textdata")
get_sentiments("afinn")
get_sentiments("afinn")
get_sentiments("loughran") %>% count(sentiment)
nrc <- get_sentiments("nrc") %>%
select(word, sentiment)
tweet_words %>% inner_join(nrc, by = "word") %>%
select(source, word, sentiment) %>% sample_n(10)
sentiment_counts <- tweet_words %>%
left_join(nrc, by = "word") %>%
count(source, sentiment) %>%
spread(source, n) %>%
mutate(sentiment = replace_na(sentiment, replace = "none"))
sentiment_counts
tweet_words %>% group_by(source) %>% summarize(n = n())
sentiment_counts %>%
mutate(Android = Android / (sum(Android) - Android) ,
iPhone = iPhone / (sum(iPhone) - iPhone),
or = Android/iPhone) %>%
arrange(desc(or))
library(broom)
log_or <- sentiment_counts %>%
mutate( log_or = log( (Android / (sum(Android) - Android)) / (iPhone / (sum(iPhone) - iPhone))),
se = sqrt( 1/Android + 1/(sum(Android) - Android) + 1/iPhone + 1/(sum(iPhone) - iPhone)),
conf.low = log_or - qnorm(0.975)*se,
conf.high = log_or + qnorm(0.975)*se) %>%
arrange(desc(log_or))
log_or
log_or %>%
mutate(sentiment = reorder(sentiment, log_or),) %>%
ggplot(aes(x = sentiment, ymin = conf.low, ymax = conf.high)) +
geom_errorbar() +
geom_point(aes(sentiment, log_or)) +
ylab("Log odds ratio for association between Android and sentiment") +
coord_flip()
android_iphone_or %>% inner_join(nrc) %>%
filter(sentiment == "disgust" & Android + iPhone > 10) %>%
arrange(desc(or))
android_iphone_or %>% inner_join(nrc, by = "word") %>%
mutate(sentiment = factor(sentiment, levels = log_or$sentiment)) %>%
mutate(log_or = log(or)) %>%
filter(Android + iPhone > 10 & abs(log_or)>1) %>%
mutate(word = reorder(word, log_or)) %>%
ggplot(aes(word, log_or, fill = log_or < 0)) +
facet_wrap(~sentiment, scales = "free_x", nrow = 2) +
geom_bar(stat="identity", show.legend = FALSE) +
theme(axis.text.x = element_text(angle = 90, hjust = 1))
library(dslabs)
library(lubridate)
options(digits = 3)
dates <- c("09-01-02", "01-12-07", "02-03-04")
ymd(dates)
mdy(dates)
dmy(dates)
data(brexit_polls)
brexit_polls%>%head
str_detect(brexit_polls$startdate, pattern = "^2016-04")
str_detect(brexit_polls$startdate, pattern = "-04-")
str_detect(brexit_polls$startdate, pattern = "\d\d\d\d-04-")
str_detect(brexit_polls$startdate, pattern = "\\d\\d\\d\\d-04-")
str_detect(brexit_polls$startdate, pattern = "\\d\\d\\d\\d-04-\d\d")
str_detect(brexit_polls$startdate, pattern = "\\d\\d\\d\\d-04-\\d\\d")
a=[FALSE,TRUE,FALSE]
polls_april <- str_detect(brexit_polls$startdate, pattern = "\\d\\d\\d\\d-04-\\d\\d")
count(polls_april)
length(polls_april)
polls_april == TRUE
is.true(polls_april)
sum(polls_april)
length(polls_april[polls_april == TRUE])
round_date(brexit_polls$enddate, unit="week", week_start = "2016-06-12")
week <- 2016-06-12
round_date(brexit_polls$enddate, unit="week",)
endweek <- round_date(brexit_polls$enddate, unit="week",)
str_detect(endweek, pattern = "week")
str_detect(brexit_polls$enddate, pattern = "week")
sum(round_date(brexit_polls$enddate, unit = "week") == "2016-06-12")
weekdays(brexit_polls$enddate, abbreviate = FALSE)
count(weekdays(brexit_polls$enddate, abbreviate = FALSE))
sum(weekdays(brexit_polls$enddate, abbreviate = FALSE))
weekdays <- weekdays(brexit_polls$enddate, abbreviate = FALSE)
count(weekdays)
group_by(weekdays)
table(weekdays)
data(movielens)
movielens%>%head
as_datetime(movielens$timestamp)
timerevi <- as_datetime(movielens$timestamp)
table(timerevi, decrease=TRUE)
table(timerevi)
movielens %>%
mutate(timestamp1 = as.POSIXct(timestamp, origin = "1970-01-01", tz = "UTC"),
year = format(timestamp1, "%Y")) %>%
count(year) %>%
arrange(desc(n))
mutate(timestamp1 = as_datetime(timestamp),
year = year(timestamp1)) %>%
group_by(year) %>%
summarise(n = n()) %>%
arrange(desc(n))
mutate(timestamp1 = as_datetime(timestamp),
year = year(timestamp1)) %>%
group_by(year) %>%
summarise(n = n()) %>%
arrange(desc(n))
library(lubridate)
movielens %>%
mutate(timestamp1 = as_datetime(timestamp),
year = year(timestamp1)) %>%
group_by(year) %>%
summarise(n = n()) %>%
arrange(desc(n))
library(lubridate)
movielens %>%
mutate(timestamp1 = as_datetime(timestamp),
time = time(timestamp1)) %>%
group_by(time) %>%
summarise(n = n()) %>%
arrange(desc(n))
movielens %>%
mutate(timest = as_datetime(timestamp),
time = time_format("%H:%M:%S", tz="UTC")) %>%
group_by(time) %>%
summarise(n=n())%>%
arrange(desc(n))
movielens %>%
mutate(timest = time_format(timestamp),
time = time(timest)) %>%
group_by(time) %>%
summarise(n=n())%>%
arrange(desc(n))
movielens %>%
mutate(timest = as_datetime(timestamp),
time = time(timest)) %>%
group_by(time) %>%
summarise(n=n())%>%
arrange(desc(n))
movielens %>%
mutate(timest = as_datetime(timestamp),
time = time(timest)) %>%
group_by(time_format(format="%H:%M:%S", tz="UTC")) %>%
summarise(n=n())%>%
arrange(desc(n))
movielens %>%
mutate(timestamp1 = as.POSIXct(timestamp, origin = "1970-01-01", tz = "UTC"),
hour = format(timestamp1, "%H")) %>%
count(hour) %>%
arrange(desc(n))
library(tidyverse)
library(gutenbergr)
library(tidytext)
options(digits = 3)
install.packages("gutenbergr")
library(tidyverse)
library(gutenbergr)
library(tidytext)
options(digits = 3)
gutenberg_metadata
str_detect(gutenberg_metadata$title, patter="Pride and Prejudice", negate=FALSE)
sum(str_detect(gutenberg_metadata$title, patter="Pride and Prejudice", negate=FALSE))
sum(str_detect(gutenberg_metadata$title, patter="Pride and Prejudice"))
gutenberg_metadata %>%
filter(str_detect(title, "Pride and Prejudice"))  %>%
summarise(n = n_distinct(gutenberg_id))
gutenberg_works()
gutenberg_works("Pride and Prejudice")
gutenberg_works(=="Pride and Prejudice")
gutenberg_works()
gutenberg_works(title=="Prime and Prejudice")
gutenberg_works(gutenberg_works$title=="Prime and Prejudice")
gutenberg_works$title
gutenberg_metadata$title
gutenberg_works(title =="Pride and Prejudice")%>%
count(gutenberg_metadata$gutenberg_id)
gutenberg_metadata %>%
filter(str_detect(title, "Pride and Prejudice"))
gutenberg_metadata %>%
gutenberg_works(filter(str_detect(title, "Pride and Prejudice")%>%
count(gutenberg_id)))
gutenberg_works()
gutenberg_works(filter(str_detect(title, "Pride and Prejudice")
)
)
gutenberg_works%>% filter(str_detect(title, "Pride and Prejudice"))
str_detect(gutenberg_works$title, "Pride and Prejudice")
gutenberg_works(title="Pride and Prejudice")%>%
count(gutenberg_id, sort=TRUE)
gutenberg_works(title=="Pride and Prejudice")%>%
count(gutenberg_id, sort=TRUE)
library(tidytext)
words <- gutenberg_download(1342)
words
count(words)
words <- gutenberg_download(1342, strip = TRUE)
words
countwords <- words %>% unnest_tokens(word, text)
head(countwords)
data("stop_words")
word_count <- countwords %>%
anti_join(stop_words, by ="word")%>%
mutate(word=str_extract(word, "[a-z']+"))%>%
count(title, word, sort=TRUE)
familiar_texts <- gutenberg_download(c(5, 1342), meta_fields = "title")
tokenized_words <- familiar_texts %>%
unnest_tokens(word, text)
head(tokenized_words)
data(stop_words)
head(stop_words)
word_counts <- tokenized_words %>%
anti_join(stop_words, by = "word") %>%
mutate(word = str_extract(word, "[a-z']+")) %>%
count(title, word, sort = TRUE)
head(word_counts)
word_proportions <- word_counts %>%
group_by(title) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
spread(title, proportion) %>%
drop_na
library(ggplot2)
word_proportions %>%
top_n(20, `Pride and Prejudice`) %>%
mutate(word = reorder(word, `Pride and Prejudice`)) %>%
ggplot(aes(word, `Pride and Prejudice`)) +
geom_col() +
xlab(NULL) +
coord_flip()
words <- gutenberg_download(1342)
words
count(word(words))
sapply(strsplit(words, " "), length)
library(sapply))
library(tidyverse)
library(sapply))
sapply(strsplit(words, " "), length)
str_count(words)
str_count(words, pattern= "")
words <- gutenberg_download(1342)
words
View(words)
View(words)
sapply(strsplit(words, " "), length
)
sapply(strsplit(words, "\\s+"), length)
stri_count(words,regex="\\S+")
words <- gutenberg_download(1342)
words
library(dplyr)
library(stringr)
library(tidytext)
tidy_book <- words %>%
unnest_tokens(word, text)
tidy_book
View(tidy_book)
pride_prejudice <- gutenberg_download(1342)
View(pride_prejudice)
View(pride_prejudice)
unnest_tokens(pride_prejudice)
unnest_tokens(pride_prejudice)
unnest_tokens(pride_prejudice['text'])
unnest_tokens(pride_prejudice[text])
tidy_book <- pride_prejudice %>%
unnest_tokens(word, text)
tidy_book <- pride_prejudice['text'] %>%
unnest_tokens(word, text)
View(tidy_book)
View(tidy_book)
cleaned_book <- tidy_books %>%
anti_join(get_stopwords())
cleaned_book <- tidy_book %>%
anti_join(get_stopwords())
install.packages('stopwords')
cleaned_book <- tidy_book %>%
anti_join(get_stopwords())
View(cleaned_book)
View(cleaned_book)
length(cleaned_book)
rows(cleaned_book)
nrows(cleaned_book)
dim(cleaned_book)
dim(tidy_book)
lapply(strsplit(pride_prejudice, "\\s+"), function(x)
x[!(duplicated(x) | duplicated(x,fromLast=TRUE))])
View(pride_prejudice)
View(pride_prejudice)
View(cleaned_book)
View(cleaned_book)
View(tidy_book)
View(tidy_book)
book <- gutenberg_download(1342)
words <- book %>%
unnest_tokens(word, text)
nrow(words)
print("hello, world")
install.packages(c("shiny", "leaflet"))
library(shiny)
library(leaflet)
r_colors <- rgb(t(col2rgb(colors()) / 255))
names(r_colors) <- colors()
ui <- fluidPage(
leafletOutput("mymap"),
p(),
actionButton("recalc", "New points"),
p(),
textOutput("coordinates")
)
server <- function(input, output, session) {
points <- eventReactive(input$recalc, {
points = cbind(rnorm(40) * 2 + 13, rnorm(40) + 48)
output$coordinates <- renderText({
points
})
return(points)
}, ignoreNULL = FALSE)
observeEvent(input$Map_shape_click, { # update the location selectInput on map clicks
output$coordinates <- renderText({
"You have selected this"
})
})
output$mymap <- renderLeaflet({
leaflet() %>%
addProviderTiles(providers$Stamen.TonerLite,
options = providerTileOptions(noWrap = TRUE)
) %>%
addMarkers(data = points())
})
}
shinyApp(ui, server)
install.packages("RMySQL", type = "source")
library("RMySQL", lib.loc="~/R/x86_64-pc-linux-gnu-library/3.5")
library("RMySQL")
setwd("~/GitHub/CourseraRepo/Reproducible_research_final")
temp <- tempfile()
download.file("https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2", destfile = "stormData.csv.bz2", method = "curl")
NOAA <- read.csv(bzfile("stormData.csv.bz2"), sep=",", header=T)
